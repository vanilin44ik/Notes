{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Model\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Imputed dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"../data/train.tsv\", sep='\\t', index_col=0, na_values=['?']).drop(\"y\", axis=1)\n",
    "test_raw = pd.read_csv(\"../data/test.tsv\", sep='\\t', index_col=0, na_values=['?'])\n",
    "dataset_raw = pd.concat([train_raw, test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = \"knn2\" # best - knn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Определение пайплайна для преобразования всего датасэта\n",
    "# pipeline_preprocessing_dataset = Pipeline([\n",
    "#     ('imputer', KNNImputer(n_neighbors=2)),    # Заполнение пропусков\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_imputed = pd.DataFrame(\n",
    "#     pipeline_preprocessing_dataset.fit_transform(dataset_raw), \n",
    "#     columns=dataset_raw.columns\n",
    "# )\n",
    "# dataset_imputed.to_csv(f\"../tmp/datasets/{imputer}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputed dataset for selection features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_imputer = \"knn1\" # best knn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Определение пайплайна для преобразования всего датасэта\n",
    "# pipeline_preprocessing_dataset = Pipeline([\n",
    "#     ('imputer', KNNImputer(n_neighbors=1)),    # Заполнение пропусков\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_imputed = pd.DataFrame(\n",
    "#     pipeline_preprocessing_dataset.fit_transform(dataset_raw), \n",
    "#     columns=dataset_raw.columns\n",
    "# )\n",
    "# dataset_imputed.to_csv(f\"../tmp/datasets/{selection_imputer}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Read **dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_imputed = pd.read_csv(f\"../tmp/datasets/{imputer}.csv\").reset_index()\n",
    "y = pd.read_csv(\"../data/train.tsv\", sep='\\t', index_col=0, na_values=['?']).y\n",
    "\n",
    "train_imputed = dataset_imputed[:train_size]\n",
    "test_imputed = dataset_imputed[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature** selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = \"catboost-1250it\" # catboost-1250it - best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_path = f\"../tmp/selected_features/{selection_imputer}_{feature_selection}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.023409\n",
      "0:\tlearn: 0.7915538\ttotal: 103ms\tremaining: 2m 8s\n",
      "250:\tlearn: 0.8901472\ttotal: 22.7s\tremaining: 1m 30s\n",
      "500:\tlearn: 0.9096114\ttotal: 44.7s\tremaining: 1m 6s\n",
      "750:\tlearn: 0.9194666\ttotal: 1m 7s\tremaining: 44.8s\n",
      "1000:\tlearn: 0.9291271\ttotal: 1m 28s\tremaining: 22.1s\n",
      "1249:\tlearn: 0.9370798\ttotal: 1m 49s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# selection_train = pd.read_csv(f\"../tmp/datasets/{selection_imputer}.csv\").reset_index()[:train_size]\n",
    "# selection_model = CatBoostClassifier(1250, task_type=\"GPU\", eval_metric='F1', verbose=250, random_state=42)\n",
    "# selection_model.fit(selection_train, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Вытаскиваем значения важности признаков\n",
    "# importances = selection_model.feature_importances_\n",
    "\n",
    "# # Создаем DataFrame с важностью признаков\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': train_imputed.columns,\n",
    "#     'Importance': importances\n",
    "# }).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance.to_csv(feature_importance_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get **important features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(feature_importance_path), \"Нет файла с такими параметрами\"\n",
    "selected_features = pd.read_csv(feature_importance_path)[\"Feature\"][:num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, selected_features):\n",
    "        \"\"\"\n",
    "        Фильтр признаков, оставляющий только указанные.\n",
    "        \n",
    "        Параметры:\n",
    "        - selected_features: pd.Series с именами отобранных признаков\n",
    "        \"\"\"   \n",
    "        self.selected_features = selected_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Отбирает только указанные признаки.\n",
    "        \"\"\"\n",
    "        return X[self.selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение пайплайна для преобразования\n",
    "pipeline_preprocessing = Pipeline([\n",
    "    ('feature_filter', FeatureFilter(selected_features=selected_features)),\n",
    "    (('standart_scaler'), StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pipeline_preprocessing.fit_transform(dataset_imputed);\n",
    "\n",
    "train = dataset[:train_size]\n",
    "test = dataset[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set **model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, *coef):\n",
    "        self.labelencoding = LabelEncoder()\n",
    "\n",
    "        self.catboost = CatBoostClassifier(\n",
    "            task_type='GPU', \n",
    "            eval_metric='F1',\n",
    "            verbose=False,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        self.lgbm_params = dict(\n",
    "            n_estimators=450,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        self.lgbm = LGBMClassifier(**self.lgbm_params)\n",
    "\n",
    "        self.xgb_params = dict(\n",
    "            n_estimators=400,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.xgb = XGBClassifier(**self.xgb_params)\n",
    "\n",
    "        self.randomforest_params = dict(\n",
    "            n_estimators=250,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.randomforest = RandomForestClassifier(**self.randomforest_params)\n",
    "\n",
    "        self.models = [self.catboost, self.lgbm, self.xgb, self.randomforest]\n",
    "        self.coef = coef\n",
    "        assert len(coef) == len(self.models)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = self.labelencoding.fit_transform(y)\n",
    "        for i, model in enumerate(self.models):\n",
    "            if self.coef[i]:\n",
    "                model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            if self.coef[i]:\n",
    "                proba += model.predict_proba(X) * self.coef[i]\n",
    "        \n",
    "        return proba\n",
    "    \n",
    "    def get_params(self):\n",
    "        result = f'{self.coef}'\n",
    "\n",
    "        if self.coef[0]:\n",
    "            result += f'\\n - CatBoost: {self.catboost.get_params()}'\n",
    "\n",
    "        if self.coef[1]:\n",
    "            result += f'\\n - LGBM: {self.lgbm_params}'\n",
    "\n",
    "        if self.coef[2]:\n",
    "            result += f'\\n - XGB: {self.xgb_params}'\n",
    "\n",
    "        if self.coef[3]:\n",
    "            result += f'\\n - RandomForest: {self.randomforest_params}'\n",
    "\n",
    "\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ensemble(0.25, 0.6, 0.05, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.445 # 0.44 best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(test)[:, 1]\n",
    "predict = np.where(proba >= threshold, 'P', 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(predict, columns=['y'])\n",
    "\n",
    "description_path = \"../subs/description.txt\"\n",
    "is_exist = os.path.exists(description_path)\n",
    "\n",
    "with open(\"../subs/description.txt\", \"r+\" if is_exist else \"w\", encoding='utf-8') as file:\n",
    "    if is_exist:\n",
    "        data = ''.join(file.readlines()).split('\\n\\n')\n",
    "\n",
    "    if is_exist and data[-1]:\n",
    "        file.write(\"\\n\\n\")\n",
    "        id = round(float(data[-1].split('\\n')[0][4:]) + 0.1, 1)\n",
    "    else:\n",
    "        id = 1.0\n",
    "\n",
    "    file.write(f\"ID: {id}\\n\")\n",
    "    file.write(f\"Impute selection - {selection_imputer}\\n\")\n",
    "    file.write(f\"Feature Selection - {feature_selection} (num-{num_features})\\n\")\n",
    "    file.write(f\"Impute - {imputer}\\n\")\n",
    "    file.write(f\"Preprocessing - {list(pipeline_preprocessing.named_steps.keys())}\\n\")\n",
    "    file.write(f\"Model - {model.__class__.__name__}\\n\")\n",
    "    file.write(f\"Params: {model.get_params()}\\n\")\n",
    "    file.write(f\"Threshold - {threshold}\\n\")\n",
    "    file.write(f\"LeaderBord Score: \")\n",
    "\n",
    "    sub.to_csv(f\"../subs/submission_{id}.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
