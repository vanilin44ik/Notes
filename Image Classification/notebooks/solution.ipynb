{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Utils\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зафиксируем seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/train'\n",
    "\n",
    "dataset, classes = reading_dataset_from_folders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../data/train.csv\")\n",
    "# path = r'../data/train'\n",
    "\n",
    "# dataset = reading_dataset_from_file(path, df.set_index('image_name')['class_id'])\n",
    "# classes = df.drop_duplicates('class_id').sort_values('class_id')['unified_class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset = split_dataset(dataset, 0.2) # Разбиваем dataset на trainset, validset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformation** and **augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим базовые преобразования \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Преобразовать в тензор\n",
    "    transforms.Normalize(mean=mean, std=std)    # Нормализовать данные\n",
    "])\n",
    "\n",
    "# Определим преобразования с аугментациями\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),                                         # Случайное горизонтальное отражение\n",
    "    transforms.RandomVerticalFlip(p=0.5),                                           # Случайное вертикальное отражение\n",
    "    transforms.RandomRotation(degrees=15),                                          # Случайный поворот на ±15 градусов\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),                       # Случайное смещение\n",
    "    transforms.RandomResizedCrop(size=image_size[1], scale=(0.8, 1.0)),             # Случайное кадрирование и изменение размера\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Случайное изменение цветов\n",
    "    transforms.RandomGrayscale(p=0.2),                                              # Случайное преобразование в оттенки серого\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),                      # Случайная перспектива\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),           # Случайное удаление части изображения\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),                       # Случайное размытие\n",
    "    transforms.ToTensor(),                                                          # Преобразовать в тензор\n",
    "    transforms.Normalize(mean=mean, std=std)                                        # Нормализовать данные\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*trainsets, validset = ( # обработка данных\n",
    "    list(map(lambda x: (t(x[0]), x[1]), tqdm(d)))\n",
    "    for d, t in [\n",
    "        (trainset, transform),\n",
    "        (trainset, augmentation),\n",
    "        (validset, transform),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = trainsets[0] + validset\n",
    "\n",
    "trainset = list()\n",
    "for temp in trainsets:\n",
    "    trainset += temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(dataset, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader для каждой выборки\n",
    "batch_size = 24\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сверточная нейронная сеть\n",
    "mymodel = nn.Sequential(\n",
    "    Conv2dBlock(image_size[0], 9),\n",
    "    Conv2dBlock(9, 16),\n",
    "    SkipConnection(\n",
    "        Conv2dBlock(16, 32, False),\n",
    "        Conv2dBlock(32, 16, False)\n",
    "    ),\n",
    "    Conv2dBlock(16, 32),\n",
    "    Conv2dBlock(32, 64),\n",
    "    nn.Flatten(),\n",
    "    LinearBlock(12544, 1500),\n",
    "    SkipConnection(\n",
    "        LinearBlock(1500, 1500)\n",
    "    ),\n",
    "    LinearBlock(1500, 500),\n",
    "    LinearBlock(500, len(classes))\n",
    ")\n",
    "\n",
    "mymodel = Model(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mymodel.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'MyModel', min_loss=True)\n",
    "scores[mymodel.best_score] = mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mymodel.load(\"models/mymodel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)  \n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, len(classes))  \n",
    "\n",
    "resnet = Model(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=3e-4)  \n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'ResNet', min_loss=True)  \n",
    "scores[resnet.best_score] = resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.load(\"models/resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)  \n",
    "vgg.classifier[6] = nn.Linear(vgg.classifier[6].in_features, len(classes))\n",
    "\n",
    "vgg = Model(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vgg.parameters(), lr=3e-4)  \n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'VGG', min_loss=True)  \n",
    "scores[vgg.best_score] = vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg.load(\"models/vgg.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, len(classes))\n",
    "\n",
    "efficientnet = Model(efficientnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(efficientnet.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'EfficientNet', min_loss=True)\n",
    "scores[efficientnet.best_score] = efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet.load(\"models/efficientnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet-201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1)\n",
    "densenet.classifier = nn.Linear(densenet.classifier.in_features, len(classes))\n",
    "\n",
    "densenet = Model(densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(densenet.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'DenseNet', min_loss=True)\n",
    "scores[densenet.best_score] = densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densenet.load(\"models/densenet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)  \n",
    "mobilenet.classifier[3] = nn.Linear(mobilenet.classifier[3].in_features, len(classes))  \n",
    "\n",
    "mobilenet = Model(mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mobilenet.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet.fit(trainloader, validloader, optimizer, loss_fn, 15, f1_macro, 'MobileNet', min_loss=True)\n",
    "scores[mobilenet.best_score] = mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet.load(\"models/mobilenet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = scores[max(scores)] # ? Выбрать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculation:')\n",
    "loss, score = best_model.evaluate(validloader, loss_fn, metrics)\n",
    "print(f\"\\nLoss: {loss:.4f}\\n\")\n",
    "print(\"Scores:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/test'\n",
    "\n",
    "testset, image_names = reading_testset(path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = best_model.predict(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
